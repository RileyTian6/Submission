---
title: "EC349 Assignment"
output: html_document
date: "2023-11-12"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/")
```

## Report
In this project, I typically used the CRISP-DM methodology, which is very intuitive. I first identify the business need that is to predict the review stars given by individual i to business j, referred to as review stars in my study. Then I clean the variables in the two small datasets to my desired format and drop  any irrelevant variables. Noted that I assume missing value in the data set is missing randomly, since it is not explicitly stated, thus I get rid of  all rows containing missing values, for the sake of both robustness and computationally. Since the review stars are transferred to factors with five levels ranging from 1 to 5, it constitutes a categorical variable. Thus, I focus on classification methodologies, including decision trees and random forests, to carry out the prediction task. In this report, I will articulate the statistical explanation behind each of these models in the context of predicting review stars, evaluate whether these approaches have yielded decent performance, primarily in terms of prediction accuracy, and raise suggestions that will potentially improve the model's overall performance. 
 
I initially employed the decision tree method, known for its recursive binary splitting, wherein each observation is assigned to the most frequently occurring class among the training observations to which it belongs (James et al, 2022). Decision tree is famous for its flexible nature, but my project did not fully exploit this feature, as the "tree" package defaulted to generating only five nodes even when the cost-complexity parameter was set only at 0.01, indicating a quite loose stopping criteria. This limitation may stem from the other variables lacking predictive power. As a result, the decision tree only incorporated two variables, namely average stars and cool.x, neglecting the potential impact of other predictors. Another problem with my data set is the significant imbalance among categories, with a Gini index at 0.7045 and star 1 and star 5 overwhelmingly dominating similar categories.  Specifically, when simplifying the review star categorization into two levels—high and low—star 5 prevails in the high level, while star 1 dominates the low level. Consequently, the decision tree assigns either 1 or 5 at the terminals. Since the decision tree continuously splits within the same training set and the proportions of each review star level remain constant, every branch leads to a binary terminal outcome. This results in a misclassification error rate of 0.4848 in the training set and a prediction accuracy of 0.511 in the test set. In fact, the tree can be pruned to only two terminal nodes, offering a more interpretable model without altering the results. Hence, according to the decision tree, individual i will give business j star 5 is the individual rate above 3.295 stars in average and give star 1 if otherwise. This would suggest that individuals have different standards, and those who are harsher than others tend to rate fewer stars. 
```{r}
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_decision_tree.png")
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_optimal_size.png")
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_pruned_decision_tree.png")
```

This result is obviously not robust, so I then utilised the random forest method, where the trees are produced from difference bootstraps that are drawn from the training set, and splits regarding different subsets of  variables in each tree (James et al, 2022). Theoretically, more bootstraps would decrease the variance in prediction, thus improving the prediction accuracy; splitting upon different subsets of variables would assure the decrease in prediction variance, otherwise, every tree in the forest will split upon the same powerful predictors in the top splits, creating similar trees and therefore, highly correlated prediction. In such a scenario, averaging would be less effective in reducing variance compared to predictions that are highly uncorrelated. In my project, there are 25 variables, so I set the number of variables at 5 in each subset. Generating 50 trees in the forest, the predictions now contain all levels from 1 to 5, whereas, the Out-of-Bag (OOB) estimate of  error rate, an alternative measure of misclassification error rate, is 0.4637 in the training set, and the prediction accuracy is 0.5411 in the test set, indicating only a marginal improvement in prediction than decision tree. However, all important predictors have been considered, among which the variable, average stars, owns the significantly highest importance according to both Mean Decrease Accuracy and Mean Decrease Gini graph, which is consistent with the pattern generated by decision tree. 

```{r Random Forest 50}
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_ransdom_forest_50.png")
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_VIP_50.png")
```

From the random forest plot, the misclassification error of half of the trees in the forest is decreasing with the number of trees, adding this to the theory that more bootstraps would decrease the prediction variance, I re-set the number of trees to 100. The OOB estimate of  error rate in the training set decreases to 0.4556, and the test prediction accuracy grows to 0.5494. After increasing the number of trees to 200, OOB estimate of  error rate in the training set decreases to 0.4521, and the test prediction accuracy grows to 0.5534. Even though the improvement is not significant, it is very promising in the sense that increasing the number of trees in the forest could be beneficial to the prediction of review stars in this project. I should have used the method of cross-validation to identify the optimal number of trees, which introduced the smallest misclassification error rate, but this was limited by the computationally of my device unfortunately. Admittedly, the huge number of complex trees in the random forest is not as easy to be interpreted as that in the context of decision tree. Albeit we know average stars from individuals is the most important predictor, the exact the relationship between this predictor and the prediction is not transparent in the forest. Given that setting the number of trees at 200 is the best I can do, I would personally prefer decision tree model in this case. After all, the random forest only slightly out-perform decision tree in prediction accuracy and this improvement comes at a significant cost in terms of interpretability.  Specially, I value the relationship between prediction and predictors are as important as the prediction itself, in the sense that this insight is crucial for businesses to grasp the nature of individuals and make strategic improvements to garner higher review stars in the future.

```{r Random Forest 100}
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_ransdom_forest_100.png")
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_VIP_100.png")
```
```{r Random Forest 200}
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_ransdom_forest_200.png")
knitr::include_graphics("/Users/tianshiwei/Desktop/Chamber of Secret/Warwick/Year 3/EC349/EC349/Project/plot_VIP_200.png")
```

Unfortunately, both models fail to give a well-performed prediction. One possible reason would be all predictors are capturing the characteristics of the individuals together with the individuals’ subjective comments. Nonetheless, crucial business-related characteristics are inadequately documented, introducing bias into the predictions. Specifically, the prominence of average star ratings as the most crucial predictor implies a focus solely on individual subjective preferences and personality. Yet, the objective conditions and the quality of service offered by businesses are inherently relevant. Therefore, including objective business characteristics measures like average stars received, parking availability, smoking areas, air conditioning, etc., would enhance the accuracy of predictions by providing a more comprehensive representation of the overall business conditions. There is no free lunch. Unbiased predictions hinge from the additional predictors will trigger interpreting difficulties, and more importantly, the problem of overfitting would arise especially when using decision tree, underscoring the importance of pruning branches in the light of cross-validation.

The most challenging difficulty is the limited computational resources. To address this, I not only used the small datasets only, omitting some relevant aforementioned variables, but also replaced advanced R packages with basic alternatives, sacrificing some functionality for efficiency. For example, I was intended to use the “rpart” package to generate the decision tree, but it takes too long for R to response, so I have to switched to a relatively basic package, the “tree” package. 

Reference

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2022). An introduction to statistical learning: With applications in R. Springer.
